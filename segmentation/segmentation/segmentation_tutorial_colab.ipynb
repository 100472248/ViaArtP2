{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YWypXSUWT0L"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZpAYDW-WT0T"
   },
   "source": [
    "# Segmentación con Deeplab-V3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGG0IcltWT0W"
   },
   "source": [
    "El propósito de este laboratorio es que el alumno se familiarice con una red neuronal convolucional (CNN) para segmentación semántica de imágenes (en la que a cada píxel de la imagen de entrada se asocia una clase de objeto). En concreto, en este tutorial se realizará _fine-tuning_ sobre la red `Deeplab-V3`. El objetivo de la práctica es que el alumno analice los bloques específicos para segmentación de la arquitectura de red, compare la segmentación binaria y la segmentación multi-clase; y trabaje sobre otros conceptos como el _data augmentation_ o la función de coste de la red neuronal en un escenario real con el objetivo de mejorar el aprendizaje de la misma. \n",
    "\n",
    "En concreto, se realizará _fine-tuning_ sobre la red `Deeplab-V3`, una de las redes del estado del arte para segmentación semántica. En nuestro caso, la tarea de segmentación a la que se va a aplicar esta arquitectura es la segmentación del disco óptico y su parte central (la cúpula óptica) en retinografía.\n",
    "\n",
    "Este tutorial es una adaptación del tutorial disponible en https://expoundai.wordpress.com/2019/08/30/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch/.\n",
    "\n",
    "Referencias:\n",
    "- [1] Deeplab-V3. https://arxiv.org/abs/1706.05587\n",
    "- [2] Imagen. https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road.jpg\n",
    "- [3] Imagen. https://raw.githubusercontent.com/abreheret/PixelAnnotationTool/master/images_test/Abbey_Road_color_mask.png\n",
    "- [4] Deeplab. https://arxiv.org/abs/1606.00915\n",
    "- [5] RIGA dataset. http://academictorrents.com/details/eb9dd9216a1c9a622250ad70a400204e7531196d\n",
    "- [6] Regularización local. http://www.sfu.ca/~abentaie/papers/miccai16.pdf\n",
    "- [7] Focal Loss for Object Detection. https://arxiv.org/abs/1708.\n",
    "- [8] Dice Loss. https://arxiv.org/abs/1606.04797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMuLiWeAWT0b"
   },
   "source": [
    "## Antes de empezar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXa7NpzOWT0c"
   },
   "source": [
    "Antes de empezar, necesita configurar algunas cosas en caso de que vaya a utilizar Google Colab. En particular, necesita descomprimir los archivos de la práctica en una carpeta en Drive y cambiar el directorio de trabajo al de dicha carpeta. Para ello, ejecute el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1980,
     "status": "ok",
     "timestamp": 1587802374458,
     "user": {
      "displayName": "MIGUEL MOLINA MORENO",
      "photoUrl": "",
      "userId": "15687095164361239457"
     },
     "user_tz": -120
    },
    "id": "5LnLkBWdWT0d",
    "outputId": "c8547771-0857-47c3-c477-5fc9e83ee405"
   },
   "outputs": [],
   "source": [
    "#Descomenta únicamente si quieres ejecutar este código en Google Colab\n",
    "#from google.colab import drive\n",
    "#import os, sys\n",
    "#drive.mount('/content/drive')\n",
    "#print(os.getcwd())\n",
    "#os.chdir('/content/drive/My Drive/Colab Notebooks/segmentation_folder') #Here put the full path to the folder where you have the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDwjc_WOWT0j"
   },
   "source": [
    "Además, si quiere ejecutar el código con soporte a GPU, en Google Colab vaya a `Entorno de ejecución->Cambiar tipo entorno de ejecución` y seleccione GPU en `acelerador por hardware`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJozOSy9WT0k"
   },
   "source": [
    "## Parte 1. Fundamento teórico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arC5lbz_WT0m"
   },
   "source": [
    "### Segmentación de objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeWE2wV2WT0m"
   },
   "source": [
    "Frente a los métodos tradicionales de segmentación de imágenes (*thresholding, clustering* o *region growing*), las técnicas de *deep learning* han demostrado ser mucho más eficaces para tareas complejas de segmentación de imágenes. Sin embargo, estas técnicas requieren grandes bases de datos anotadas píxel a píxel para su entrenamiento, lo que supone un gran esfuerzo de anotación. \n",
    " \n",
    "El objetivo de un algoritmo de segmentación de objetos es generar máscaras de salida a nivel de píxel en las cuales a las regiones que pertenecen a ciertas categorías se les asigna el mismo valor de píxel. Si se codifican en color (asignando un color diferente a cada clase de objetos) se obtienen resultados como los que se muestran en la siguiente figura [2-3], donde en azul se representa la clase vehículo, en rojo la clase persona, etc.\n",
    "\n",
    "<table><tr><td><img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/beatles.jpg\"></td><td><img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/beatles_mask.png\"></td></tr></table>\n",
    "\n",
    "Por tanto, como entrada a nuestro algoritmo de segmentación se tendrá un conjunto de imágenes y sus correspondientes máscaras *ground truth* píxel a píxel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OdggFQg8WT0n"
   },
   "source": [
    "### Contexto vs resolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxNRCl3SWT0o"
   },
   "source": [
    "El principal reto en segmentación semántica es encontrar un compromiso entre la importancia del contexto global de la imagen (para segmentar un objeto, es necesario identificar sus distintas partes y diferenciarlo del resto de objetos de la imagen), y las características locales de la imagen (una segmentación precisa ha de analizar los valores de los píxeles que se encuentran en los alrededores del objeto para delimitar la frontera del mismo). La siguiente figura muestra las distintas estrategias que se utilizan para intentar representar este compromiso sobre CNNs.\n",
    "\n",
    "<img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/arquitecturas.PNG\">\n",
    "\n",
    "La aproximación tradicional que se ha seguido para utilizar extraer características de alto nivel de la imagen ha sido el análisis de la misma a través de una representación en pirámide, en la que se consideran la imagen original y versiones de menor tamaño de la misma (filtrado gaussiano y reducción del tamaño mediante submuestreo). Si se aplica el mismo procesado a todos los niveles de la pirámide, se obtienen características de bajo nivel en los primeros niveles, y de alto nivel conforme se va profundizando en la misma (véase la figura a) arriba, donde se puede ver que se mezclan las características de dos pirámides de imagen que parten de distintas escalas). De esta manera, en las versiones de menor tamaño pierde importancia la información local de la imagen en favor de la información global, y viceversa. Las arquitecturas de redes neuronales convolucionales (FCNs) de clasificación se basan en esta representación para intentar resumir la información de la imagen (las características a niveles menos profundos sirven para calcular características a niveles más profundos) y proporcionar su categoría. \n",
    "\n",
    "Por el contrario, en segmentación no solo importa el contexto global de la imagen, sino que las características locales también cuentan. Las redes totalmente convolucionales para segmentación (*Fully Convolutional Networks*, FCNs) introducen una estructura *encoder-decoder* en la red para obtener una salida con una resolución igual (o lo más cercana posible) a la de la entrada (véase la figura b) arriba). Para ello, el *encoder* reduce paulatinamente la dimensión de los mapas de características de manera que la información global se captura en las capas profundas; y el *decoder* parte de esta información global y paulatinamente recupera la dimensión original de la imagen. Frecuentemente se incluyen conexiones entre las capas de igual dimensión del *encoder* y el *decoder* para facilitar el empleo directo de las características más locales en la salida final de la segmentación.\n",
    "\n",
    "En las redes Deeplab [1]-[4] se hace uso de las dos estrategias de la derecha: convoluciones *atrous* y *Spatial Pyramid Pooling*. Estas estrategias se explican en la siguiente sección.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UF3lO9PIWT0q"
   },
   "source": [
    "### Deeplab-V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHk4T87XWT0r"
   },
   "source": [
    "Aunque se recomienda echar un vistazo al artículo sobre Deeplab-V3 [1], en esta sección se van a explicar los conceptos más importantes de la red que son necesarios para el desarrollo de la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2nui-a8WT0s"
   },
   "source": [
    "Deeplab-V3 es un _framework_ que adapta cualquier red convolucional dedicada a la clasificación (dada una imagen, encontrar una categoría que describa el contenido total de la imagen) a la segmentación de objetos. Para ello, a partir de un _backbone_ inicial (las capas destinadas a la extracción de características) de cualquier red de clasificación, propone una serie de capas y bloques destinados a extraer características de contexto en la imagen sin comprometer la resolución de los mapas de características (sin reducir aun más su resolución). Para ello, hace uso de las dos estrategias que se describen a continuación.\n",
    "\n",
    "#### Convoluciones *atrous*\n",
    "\n",
    "La contribución principal de la familia de redes Deeplab [4] es el diseño de las convoluciones *atrous* o convoluciones *dilated*. Este tipo de convoluciones se utilizan para reemplazar la estrategia tradicional de reducir el tamaño de la imagen a través de capas de *max pooling* con *stride* a lo largo de la red para obtener representaciones más globales del contenido de la imagen. Esta estrategia estándar hace que el *stride* acumulado a lo largo de la red sea muy elevado (32, por ejemplo, lo que significa que el tamaño de las características de una imagen original de $HxW$ es $H/32xW/32$), lo que puede ser contraproducente en segmentación (a pesar de obtener mejores características de contexto, la reducción de las dimensiones de la imagen hace que las segmentaciones sean menos precisas). Las convoluciones *atrous*, por su parte, mantienen la resolución de la entrada a la vez que extraen características de mayor orden a través del uso del *stride*. Es decir, para computar el valor de un cierto píxel $y[i]$ se toman los valores de los píxeles alejados del mismo $r$ posiciones en la entrada $x$ multiplicados por el elemento correspondiente del filtro $w$.\n",
    "\n",
    "\\begin{equation}y[i]=\\sum_{k}{x[i+r\\cdot k] w[k]}.\\end{equation}\n",
    "\n",
    "En modo filtro, una convolución *atrous* con tasa $r$ se consigue añadiendo $r-1$ ceros entre los elementos del filtro original, como se muestra en la siguiente figura [1]:\n",
    "\n",
    "<img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/atrous.PNG\" width=\"600pix\">\n",
    "\n",
    "A continuación se puede observar una animación en la que se comparan una convolución 2D tradicional con una convolución *atrous*, ambas con el mismo *field of view*, pero la primera usa 15 parámetros y la segunda 9 parámetros. Mientras que la convolución estándar usa un filtro de $5x5$, *stride* de 1, *dilate* de 1 y *padding* de 1; la convolución *atrous* usa un filtro de $3x3$, *stride* $r=2$, *dilate* de 2 y no usa *padding*. \n",
    "\n",
    "<table><tr><td><img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/2dconv.gif\"></td><td><img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/atrousconv.gif\"></td></tr></table>\n",
    "\n",
    "En la práctica, estas convoluciones no se utilizan a lo largo de toda la red: en primer lugar, porque reducir el tamaño de la imagen en las primeras capas de la red es útil desde el punto de vista del coste computacional (sería muy costoso trabajar con las imágenes a tamaño completo durante todo el procesado); y en segundo lugar, porque de hecho esta sustitución del *max pooling* por convoluciones *atrous* es más útil en capas profundas de la red (con menos tasa $r$ se recorren grandes porciones de imagen). Un ejemplo de flujo de trabajo sobre ResNet se muestra a continuación [1]:\n",
    "\n",
    "<img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/atrous2.PNG\">\n",
    "\n",
    "Se puede ver como a partir del bloque 3 se utilizan convoluciones *atrous* __en cascada__ con tasa variable $2^n$ para sustituir al *max pooling* con tasa 2. De este modo se mantiene la resolución en valores aceptables. \n",
    "\n",
    "#### *Atrous Spatial Pyramid Pooling*\n",
    "\n",
    "La segunda contribución del artículo [1] tiene que ver con el uso de las convoluciones *atrous* como extractores de características __en paralelo__ con distinta tasa $r$, de manera que las características que se extraen en cada rama sean más globales o locales en función de dicha tasa. Esto es lo que se llama ASPP (*Atrous Spatial Pyramid Pooling*). Las características de cada rama se pueden agrupar bien mediante suma o bien mediante concatenación, para obtener después la salida final de segmentación de la red.\n",
    "\n",
    "En concreto, la estructura del *frawework* Deeplab-V3 es la que se muestra a continuación:\n",
    "\n",
    "<img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/aspp.PNG\">\n",
    "\n",
    "Se puede observar que se parte de un mapa de características de la imagen con *stride* 16 al que se añade una capa con convoluciones *atrous* con tasa $r=2$ y posteriormente el bloque ASPP. Este consta de los siguientes módulos:\n",
    "\n",
    "- Una capa con una convolución 1x1 que extrae características más locales del mapa de entrada.\n",
    "- Sendas convoluciones *atrous* con tasas $r=6$, $r=12$ y $r=18$, respectivamente, que extraen características con diferente contexto global de los mapas de entrada.\n",
    "- Un *avg pooling* a nivel de mapa de entrada que genera como salida la media de cada canal. Esta característica global de la imagen permite ponderar la importancia de los distintos canales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3Iv8idtWT0t"
   },
   "source": [
    "### Medidas de evaluación \n",
    "\n",
    "Para evaluar la calidad en la segmentación de objetos, se suele utilizar la medida *Intersection over Union*, IoU, también denominada *Jaccard Index* (JI). La medida $IoU$ mide la similitud entre dos regiones $A$ y $B$ como:\n",
    "\n",
    "\\begin{equation}\n",
    "IoU=\\frac{A \\cap B}{A \\cup B}\n",
    "\\end{equation}\n",
    "\n",
    "siendo $\\cap$ la intersección entre las regiones (area común) y $\\cup$ la unión o área total que cubren entre ambas. Se considera un umbral mínimo de IoU en torno a $IoU_{th}=0.7$ para considerar una detección como correcta. A continuación se puede ver un ejemplo de la medida $IoU$.\n",
    "\n",
    "<img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/iou.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNLzVweaWT0t"
   },
   "source": [
    "### Utilidad de la tarea y base de datos\n",
    "\n",
    "El glaucoma es una de las mayores causas de ceguera irreversible en el mundo. Los modelos actuales de atención médica no son capaces de acortar distancias entre la prevalencia progresiva del glaucoma y los retos para el acceso a la atención médica. La tele-oftalmología y los sistemas de ayuda al diagnóstico basados en *deep-learning* pueden ayudar a acortar esta distancia. En concreto, la retinografía o fotografía del fondo de ojo es la mejor técnica para el estudio de la papila óptica, con el objetivo detectar y evaluar síntomas del glaucoma. Los estudios que se realizan se basan en las cinco reglas siguientes:\n",
    "\n",
    "1. Observar el anillo escleral para identificar los límites del disco óptico y su tamaño.\n",
    "2. Identificar el tamaño del anillo.\n",
    "3. Examinar la capa de fibras del nervio óptico.\n",
    "4. Examinar por fuera la región del disco óptico en busca de atrofia parapapilar.\n",
    "5. Observar si hay hemorragias retinales o del disco óptico.\n",
    "\n",
    "Para estas tareas es fundamental localizar de manera precisa el disco óptico y la cúpula óptica.\n",
    "\n",
    "El disco óptico, papila óptica o punto ciego es una zona circular situada en el centro de la retina, por donde salen del ojo los axones de las células ganglionares de la retina que forman el nervio óptico. Esta área mide 1.5 x 2.5 mm en el ojo humano y carece de sensibilidad a los estímulos luminosos por no poseer ni conos ni bastones, ello causa una zona ciega dentro del campo visual que se conoce como punto ciego. Dentro de la papila se encuentra una excavación fisiológica llamada cúpula, en el centro de la misma. La siguiente figura, a la izquierda, muestra una animación con el disco óptico en color rosado y la cúpula óptica en color blanco. A la derecha, se puede ver un ejemplo de anotación del disco óptico y la cúpula óptica. \n",
    "\n",
    "<table><tr><td><img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/animation.gif\" width=\"500pix\"></td><td><img src=\"http://tsc.uc3m.es/~mmolina/images_segmentation/annotation.jpg\" width=\"500pix\"></td></tr></table>\n",
    "\n",
    "El objetivo de esta práctica será segmentar el disco y la cúpula ópticas en retinografía. En un entorno real esto es un paso previo a la extracción de características: por ejemplo, el cociente entre el diámetro de la cúpula y el diámetro del disco óptico es un indicador del daño que origina el glaucoma; y el diagnóstico.\n",
    "\n",
    "La base de datos de la que se dispone corresponde a una versión procesada de la base de datos disponible en [5]. Esta base de datos contiene 698 imágenes de retinografía procedentes de 3 proyectos diferentes: MESSIDOR, Magrabia y BinRushed. Las anotaciones de la cúpula y el disco ópticos con las que se cuenta para cada imagen se han obtenido a partir de la concordancia entre las anotaciones de 6 oftalmólogos expertos para cada caso. \n",
    "\n",
    "Los conjuntos se distribuyen de la siguiente manera:\n",
    "- Entrenamiento: 400 imágenes con su anotación correspondiente.\n",
    "- Validación: 148 imágenes con su anotación correspondiente.\n",
    "- Test: 150 imágenes con su anotación correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyDNj8ZSWT0u"
   },
   "source": [
    "## Parte 2. Implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jfzUxaUWT0v"
   },
   "source": [
    "En primer lugar, se importan las librerías necesarias y se definen algunos parámetros generales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1971,
     "status": "ok",
     "timestamp": 1587802374459,
     "user": {
      "displayName": "MIGUEL MOLINA MORENO",
      "photoUrl": "",
      "userId": "15687095164361239457"
     },
     "user_tz": -120
    },
    "id": "GqOkBNIVWT0x",
    "outputId": "4e3c6c3d-9a27-49ef-9050-67c9233aebf8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as FT\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "# torch uses some non-deterministic algorithms\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUND5DkEWT01"
   },
   "source": [
    "### Entradas\n",
    "\n",
    "Se definen algunas entradas para la ejecución:\n",
    "\n",
    "-  **data_dir** - el directorio raíz de la base de datos, que se describe posteriormente.\n",
    "-  **num_classes_binary** - el número de clases que detectar en el problema binario.\n",
    "-  **class_names_binary** - los nombres de las clases que detectar en el problema binario.\n",
    "-  **num_classes_multiclass** - el número de clases que detectar en el problema multi-clase.\n",
    "-  **class_names_multiclass** - los nombres de las clases que detectar en el problema multi-clase.\n",
    "-  **img_size** - el tamaño de las imágenes de entrada a la red (cuadradas).\n",
    "-  **batchsize_train** - el tamaño de _batch_ que se utiliza para entrenamiento.\n",
    "-  **batchsize_test** - el tamaño de _batch_ que se utiliza para test.\n",
    "-  **epochs** - número de _epochs_ para el entrenamiento de la red.\n",
    "-  **step_size** - número de *epochs* tras los cuales se reduce el *learning rate* en un factor 0.1.\n",
    "-  **result_dir** - el directorio raíz para almacenar los resultados.\n",
    "-  **device** - el dispositivo (GPU o CPU) para la ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YnmJp7nWT02"
   },
   "outputs": [],
   "source": [
    "data_dir = 'riga'\n",
    "num_classes_binary=2 # First, we use Deeplab-V3 for binary segmentation\n",
    "class_names_binary=['background','optic_cup']\n",
    "num_classes_multiclass=3 \n",
    "class_names_multiclass=['background', 'optic_cup', 'optic_disk']\n",
    "img_size=512           # Size of the images\n",
    "batchsize_train=2      # Batch size for training\n",
    "batchsize_test=1       # Batch size for test (it must be one to generate predictions)\n",
    "epochs = 8             # Number of epochs to train (must be pair)\n",
    "step_size=5\n",
    "result_dir = 'results' # Result directory\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXWUkRkFWT06"
   },
   "source": [
    "### Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23Vwm78dWT07"
   },
   "source": [
    "La red a utilizar será una versión de Deeplab-V3 con ResNet-101 como *backbone*, ya pre-entrenada sobre el conjunto de entrenamiento de la base de datos COCO. En concreto, se utilizan los 4 primeros bloques de esta red, compuestos de capas convolucionales, normalización de batch (cuyos parámetros se congelan) y capas no lineales. Vea como los bloques se modifican añadiendo convoluciones *atrous* en lugar de *max pooling*, como se ha descrito en el apartado teórico, y a partir del cuarto bloque se añade el bloque de ASPP y la convolución que genera la salida de segmentación.\n",
    "\n",
    "A continuación se incluye la función que construye la red. Analice la arquitectura.\n",
    "\n",
    "- ¿Se realiza el submuestreo con capas de *max pooling* en las capas iniciales, y *layers* 1, 2 y 3? ¿Por qué?\n",
    "- ¿Coinciden las tasas $r$ de las convoluciones *atrous* de los bloques 4 y el ASPP con el apartado teórico? ¿Por qué? ¿Cuál es el stride acumulado hasta la capa 4?\n",
    "- ¿Cuál es la diferencia en cuanto a los parámetros entre una convolución *atrous* (mantiene el tamaño de la imagen) y las convolución que sustituyen al *max pooling*?\n",
    "\n",
    "__**IMPORTANTE:__ no preste atención al `aux_classifier` de momento. Se trata de un clasificador auxiliar que se utiliza para mejorar el problema del *vanishing gradient*. Si se introduce una función de pérdida a su salida y se realiza la retropropagación, este bloque introduce gradientes más robustos en un punto intermedio de la red que pueden ayudar al entrenamiento (dado que la arquitectura es muy profunda, los gradientes de la salida estándar se van desvaneciendo a medida que se va avanzando desde la salida del clasificador estándar hasta el inicio de la red). Por el momento no se va a utilizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztVBZ1p5WT08"
   },
   "outputs": [],
   "source": [
    "def get_deeplabv3(num_classes=1):\n",
    "    model = deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "    model.classifier = DeepLabHead(2048, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3721,
     "status": "ok",
     "timestamp": 1587802376230,
     "user": {
      "displayName": "MIGUEL MOLINA MORENO",
      "photoUrl": "",
      "userId": "15687095164361239457"
     },
     "user_tz": -120
    },
    "id": "n8zSVc_HWT1B",
    "outputId": "c9aef155-ed59-455b-950b-620a9530cd34"
   },
   "outputs": [],
   "source": [
    "model = get_deeplabv3(num_classes_binary)\n",
    "print(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SnwngOisWT1G"
   },
   "source": [
    "__El modelo no es el mismo estudiado en el apartado teórico. En concreto hay una convolución con stride y max-pooling en el bloque 0 (rápido se reduce el tamaño en 4), en la layer 1 no existe downsample, en la layer 2 sí y en la layer 3 no, de manera que el stride acumulado hasta el bloque 4 es 8. Por ello, el bloque 4 contiene dos convoluciones *atrous*, una con $r=2$ y otra con $r=4$ (para aumentar el *receptive field* a la entrada del ASPP). Además, en el ASPP se consideran tasas $r=12, 24, 36$, el doble de las del apartado teórico__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YyDtaqDWT1J"
   },
   "source": [
    "### Base de datos\n",
    "\n",
    "En esta práctica se va a realizar tanto segmentación binaria como segmentación multi-clase. La base de datos se proporciona en forma de imágenes de retinografía y máscaras de objetos. Se proporcionan dos tipos de máscara: las carpetas `masks`, que contienen máscaras donde solo aparece la cúpula óptica (clase 1), que utilizaremos durante la primera parte de la práctica; y las carpetas `masks_full` donde aparecen la cúpula óptica (clase 1) y el disco óptico (clase 2). \n",
    "\n",
    "- Segmentación binaria: las máscaras que se proporcionan en este caso suelen ser imágenes `uint8` con el valor 0 para la clase `background` y 255 para la clase `optic_cup`.\n",
    "\n",
    "- Segmentación multi-clase: hay que modificar la clase de carga de red y la normalización de los datos (en este caso cada clase corresponde a un entero distinto en la máscara): 0 para la clase `background`, 1 para la clase `optic_cup` y 2 para la clase `optic_disk`.\n",
    "\n",
    "A continuación se define la clase que implementa la carga de la base de datos para ambos casos. Preste atención a las opciones `maskFolder` y `binary` para escoger entre una y otra.\n",
    "\n",
    "__**IMPORTANTE:__ para acelerar los experimentos si fuera necesario, se proporciona código para seleccionar el número de muestras con las que se entrena la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDoefXRjWT1K"
   },
   "outputs": [],
   "source": [
    "class RigaDataset(Dataset):\n",
    "    \"\"\"Binary Segmentation Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, imageFolder, maskFolder, binary=True, use_only_train=[], transform=None, subset=None,  imagecolormode='rgb', maskcolormode='grayscale'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images and should have the following structure.\n",
    "            root\n",
    "            --Images\n",
    "            -----Img 1\n",
    "            -----Img N\n",
    "            --Mask\n",
    "            -----Mask 1\n",
    "            -----Mask N\n",
    "            imageFolder (string) = 'Images' : Name of the folder which contains the Images.\n",
    "            maskFolder (string)  = 'Masks : Name of the folder which contains the Masks.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            subset: 'Train', 'Val' or 'Test' to select the appropriate set.\n",
    "            imagecolormode: 'rgb' or 'grayscale'\n",
    "            maskcolormode: 'rgb' or 'grayscale'\n",
    "        \"\"\"\n",
    "        self.color_dict = {'rgb': 1, 'grayscale': 0}\n",
    "        assert(imagecolormode in ['rgb', 'grayscale'])\n",
    "        assert(maskcolormode in ['rgb', 'grayscale'])\n",
    "\n",
    "        self.imagecolorflag = self.color_dict[imagecolormode]\n",
    "        self.maskcolorflag = self.color_dict[maskcolormode]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.binary=binary\n",
    "        assert(subset in ['Train', 'Val', 'Test'])\n",
    "        if (subset=='Train'):\n",
    "            self.image_names = sorted(glob.glob(os.path.join(self.root_dir, 'train', imageFolder, '*')))\n",
    "            self.mask_names = sorted(glob.glob(os.path.join(self.root_dir, 'train', maskFolder, '*')))\n",
    "            if (len(use_only_train)>1):\n",
    "                self.image_names=[self.image_names[x] for x in use_only_train]\n",
    "                self.mask_names=[self.mask_names[x] for x in use_only_train]\n",
    "        elif(subset=='Val'):\n",
    "            self.image_names = sorted(glob.glob(os.path.join(self.root_dir, 'val', imageFolder, '*')))\n",
    "            self.mask_names = sorted(glob.glob(os.path.join(self.root_dir, 'val', maskFolder, '*')))\n",
    "        else:\n",
    "            self.image_names = sorted(glob.glob(os.path.join(self.root_dir, 'test', imageFolder, '*')))\n",
    "            self.mask_names = sorted(glob.glob(os.path.join(self.root_dir, 'test', maskFolder, '*')))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        if self.imagecolorflag:\n",
    "            image = cv2.imread(\n",
    "                img_name, self.imagecolorflag).transpose(2, 0, 1)\n",
    "        else:\n",
    "            image = cv2.imread(img_name, self.imagecolorflag)\n",
    "        msk_name = self.mask_names[idx]\n",
    "        if self.maskcolorflag:\n",
    "            mask = cv2.imread(msk_name, self.maskcolorflag).transpose(2, 0, 1)\n",
    "        else:\n",
    "            mask = cv2.imread(msk_name, self.maskcolorflag)\n",
    "            if (self.binary):\n",
    "                # In a binary dataset, the masks are usually given in uint8 form: 0-background, 255 foreground\n",
    "                # For the BCELoss, we need the background mask (inverse of the foreground one)\n",
    "                mask2 = 255-mask\n",
    "                # We concatenate both\n",
    "                mask = np.concatenate((mask2[np.newaxis,:,:],mask[np.newaxis,:,:]),axis=0)\n",
    "        sample = {'image': image, 'mask': mask, 'img_path': img_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Define few transformations for the Segmentation Dataloader\n",
    "class Augm(object):\n",
    "    \"\"\"Perform data augmentation. NOT IMPLEMENTED YET.\"\"\"\n",
    "    def __init__(self, data_augm):\n",
    "        self.data_augm = data_augm\n",
    "    def __call__(self, sample):\n",
    "        img, mask, img_path = sample['image'], sample['mask'], sample['img_path']\n",
    "        if (self.data_augm):\n",
    "            # DATA AUGMENTATION: IMPLEMENT YOUR VERSION\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return {'image': img,\n",
    "                'mask': mask, \n",
    "                'img_path': img_path}\n",
    "\n",
    "class Resize(object):\n",
    "    \"\"\"Resize image and/or masks.\"\"\"\n",
    "\n",
    "    def __init__(self, imageresize, maskresize):\n",
    "        self.imageresize = imageresize\n",
    "        self.maskresize = maskresize\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, img_path = sample['image'], sample['mask'], sample['img_path']\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(1, 2, 0)\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask.transpose(1, 2, 0)\n",
    "        mask = cv2.resize(mask, self.maskresize, interpolation=cv2.INTER_NEAREST)\n",
    "        image = cv2.resize(image, self.imageresize, interpolation=cv2.INTER_AREA)\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask.transpose(2, 0, 1)\n",
    "\n",
    "        return {'image': image,\n",
    "                'mask': mask,\n",
    "                'img_path': img_path}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, maskresize=None, imageresize=None):\n",
    "        image, mask, img_path = sample['image'], sample['mask'], sample['img_path']\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.reshape((1,)+image.shape)\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'mask': torch.from_numpy(mask), \n",
    "                'img_path': img_path}\n",
    "\n",
    "\n",
    "class NormalizeBinary(object):\n",
    "    '''Normalize image'''\n",
    "    def __init__(self, image_mean,image_std):\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "    def __call__(self, sample):\n",
    "        image, mask, img_path = sample['image'], sample['mask'], sample['img_path']\n",
    "        # Binary segmentation: masks can be float with 0-1 values\n",
    "        if (self.image_mean is None):\n",
    "            return {'image': image.type(torch.FloatTensor)/255,\n",
    "                    'mask': mask.type(torch.FloatTensor)/255, \n",
    "                    'img_path': img_path}\n",
    "        else:   \n",
    "            return {'image': F.normalize(image.type(torch.FloatTensor)/255,self.image_mean,self.image_std),\n",
    "                    'mask': mask.type(torch.FloatTensor)/255, \n",
    "                    'img_path': img_path}\n",
    "\n",
    "        \n",
    "class NormalizeMulticlass(object):\n",
    "    '''Normalize image'''\n",
    "    def __init__(self, image_mean,image_std):\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "    def __call__(self, sample):\n",
    "        image, mask, img_path = sample['image'], sample['mask'], sample['img_path']\n",
    "        if (self.image_mean is None):\n",
    "            # Masks must be LongTensor with 0-1-2... indicators for the classes\n",
    "            return {'image': image.type(torch.FloatTensor)/255,\n",
    "                    'mask': mask.type(torch.LongTensor), \n",
    "                    'img_path': img_path}\n",
    "        else:   \n",
    "            return {'image': F.normalize(image.type(torch.FloatTensor)/255,self.image_mean,self.image_std),\n",
    "                    'mask': mask.type(torch.LongTensor), \n",
    "                    'img_path': img_path}\n",
    "\n",
    "def get_dataloader_riga(data_dir, imageFolder='images', maskFolder='masks', binary=True, use_only_train=[], batch_size=4, img_size=256, data_augm=False, image_mean=None,image_std=None):\n",
    "    \"\"\"\n",
    "        Create training, validation and testing dataloaders from a single folder.\n",
    "    \"\"\"\n",
    "    if (binary):\n",
    "        data_transforms = {\n",
    "            'Train': transforms.Compose([Augm(data_augm), Resize((img_size,img_size),(img_size,img_size)), ToTensor(), NormalizeBinary(image_mean,image_std)]),\n",
    "            'Val': transforms.Compose([Resize((img_size,img_size),(img_size,img_size)), ToTensor(), NormalizeBinary(image_mean,image_std)]),\n",
    "            'Test': transforms.Compose([Resize((img_size,img_size),(img_size,img_size)), ToTensor(), NormalizeBinary(image_mean,image_std)]),\n",
    "            }\n",
    "        image_datasets = {x: RigaDataset(data_dir, imageFolder=imageFolder, maskFolder=maskFolder, binary=binary, use_only_train=use_only_train, subset=x, transform=data_transforms[x])\n",
    "                          for x in ['Train', 'Val', 'Test']}\n",
    "        \n",
    "    else:\n",
    "        data_transforms = {\n",
    "            'Train': transforms.Compose([Augm(data_augm), Resize((img_size,img_size),(img_size,img_size)), ToTensor(), NormalizeMulticlass(image_mean,image_std)]),\n",
    "            'Val': transforms.Compose([Resize((img_size,img_size),(img_size,img_size)), ToTensor(), NormalizeMulticlass(image_mean,image_std)]),\n",
    "            'Test': transforms.Compose([Resize((img_size,img_size),(img_size,img_size)), ToTensor(), NormalizeMulticlass(image_mean,image_std)]),\n",
    "            }\n",
    "        image_datasets = {x: RigaDataset(data_dir, imageFolder=imageFolder, maskFolder=maskFolder, binary=binary, use_only_train=use_only_train, subset=x, transform=data_transforms[x])\n",
    "                          for x in ['Train', 'Val', 'Test']}\n",
    "\n",
    "    dataloaders = {'Train': DataLoader(image_datasets['Train'], batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=8),\n",
    "                   'Val': DataLoader(image_datasets['Val'], batch_size=batch_size,\n",
    "                                 shuffle=False, num_workers=8),\n",
    "                   'Test': DataLoader(image_datasets['Test'], batch_size=batch_size,\n",
    "                                 shuffle=False, num_workers=8)}\n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6EdDp7VWT1N"
   },
   "source": [
    "Se carga la base de datos, ya que en la primera parte de la práctica trabajaremos con segmentación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5byfUt6nWT1O"
   },
   "outputs": [],
   "source": [
    "# Use the generic mean if it is not given\n",
    "if (not os.path.exists(os.path.join(data_dir,'mean-channel.npy'))):\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "else:\n",
    "    mean=np.load(os.path.join(data_dir,'mean-channel.npy'))\n",
    "    std=np.load(os.path.join(data_dir,'std-channel.npy'))\n",
    "    \n",
    "# Code to use only XX images of the database for train\n",
    "#n_train=XX\n",
    "#np.random.seed(manualSeed)\n",
    "#ids=np.random.permutation(400)\n",
    "#use_only_train=ids[:n_train] #\n",
    "use_only_train=[] # if we want to use all images\n",
    "dataloaders = get_dataloader_riga(\n",
    "    data_dir, maskFolder='masks', binary=True, use_only_train=use_only_train, batch_size=batchsize_train, img_size=img_size, data_augm=False, image_mean=mean,image_std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VurAgkh3WT1T"
   },
   "source": [
    "### Medidas de evaluación\n",
    "\n",
    "Asimismo, se define la función que testea la bondad de nuestro modelo de segmentación binario en términos de segmentación (índice Jaccard o *Intersection over Union*).\n",
    "\n",
    "La función recibe como parámetros:\n",
    "\n",
    "- __model__: la CNN que evaluar.\n",
    "- __dataloader__: el cargador de los datos de test.\n",
    "- __class_names__: nombres de las clases de objetos a detectar (siempre hay que incluir en primer lugar la clase *background*).\n",
    "- __TH__: la red proporciona un *score* que convertimos a probabilidad *softmax* para cada píxel. Este umbral distingue las dos clases.\n",
    "- __result_dir__: el directorio donde guardar los resultados.\n",
    "- __SAVE_OPT__: para guardar o no los resultados de test como imágenes con el *ground truth* en verde y la segmentación en rojo, en el directorio `predictions_binary`.\n",
    "- __batchsize__: debe ser igual a 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JcRJPp5WT1U"
   },
   "outputs": [],
   "source": [
    "def test_segmentation_model_binary(model, dataloaders, class_names, TH, result_dir, SAVE_OPT, batch_size=1):\n",
    "    # Evaluation: Jaccard Index\n",
    "    jaccard=[]\n",
    "    # We create the results folder and CSV file for results if they do not exist\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    else:\n",
    "        if os.path.exists(os.path.join(result_dir,'results_binary.csv')):\n",
    "            os.remove(os.path.join(result_dir,'results_binary.csv'))\n",
    "    if (SAVE_OPT):\n",
    "        # We create the folder for predictions\n",
    "        if not os.path.exists(os.path.join(result_dir,'predictions_binary')):\n",
    "            os.mkdir(os.path.join(result_dir,'predictions_binary'))\n",
    "    csv_file=open(os.path.join(result_dir,'results_binary.csv'),'w')\n",
    "    coord_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for sample in dataloaders['Test']:\n",
    "        with torch.no_grad():\n",
    "            inputs = sample['image'].to(device)\n",
    "            masks = sample['mask'].to(device)\n",
    "            prediction = model(inputs)\n",
    "            y_pred = prediction['out']\n",
    "            y_pred = torch.nn.functional.softmax(y_pred, dim=1)\n",
    "            y_pred = y_pred[:,1,:,:].data.cpu().numpy()\n",
    "            y_true = masks.data.cpu().numpy()\n",
    "            y_true = y_true[:,1,:,:]\n",
    "            for j in range(y_pred.shape[0]):\n",
    "                img_name=sample['img_path'][j].split(os.path.sep)[-1]\n",
    "                dataset_name=sample['img_path'][j].split(os.path.sep)[-2]\n",
    "                # We measure with the Jaccard Index\n",
    "                ji=np.sum(np.logical_and(np.squeeze(y_pred[j,:,:]>TH),np.squeeze(y_true[j,:,:]>0)))/(np.sum(y_pred[j,:,:]>TH)+np.sum(y_true[j,:,:]>0)-np.sum(np.logical_and(np.squeeze(y_pred[j,:,:]>TH),np.squeeze(y_true[j,:,:]>0))))\n",
    "                jaccard.append(ji)\n",
    "                if (SAVE_OPT):\n",
    "                    img=Image.open(sample['img_path'][j]).resize((img_size,img_size))\n",
    "                    mask=(np.transpose(np.concatenate(((y_pred[j,:,:]>TH)[np.newaxis,:,:],y_true[j,:,:][np.newaxis,:,:],np.zeros_like(y_pred[j,:,:][np.newaxis,:,:])),axis=0),(1,2,0))*255.0).astype(np.uint8)\n",
    "                    img = Image.blend(img.convert('RGBA'), Image.fromarray(mask).convert('RGBA'),0.5)\n",
    "                    img.save(os.path.join(result_dir,'predictions_binary',img_name[:-4]+'.png'))\n",
    "                coord_writer.writerow([img_name,str(ji)])\n",
    "    # Mean values\n",
    "    coord_writer.writerow(['MEAN',str(sum(jaccard)/len(jaccard))])\n",
    "    print('Jaccard index for segmentation: {}'.format(sum(jaccard)/len(jaccard)))\n",
    "    csv_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pd_shP34WT1X"
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "\n",
    "La función recibe como parámetros:\n",
    "\n",
    "- __model__: la CNN que evaluar.\n",
    "- __dataloader__: el cargador de los datos de test.\n",
    "- __device__: el dispositivo que utilizar para el entrenamiento (GPU o CPU).\n",
    "- __optimizer__: el optimizador.\n",
    "- __lr_scheduler__: la política de modificación de la tasa de aprendizaje.\n",
    "- __metrics__: las métricas para medir el rendimiento del sistema de segmentación.\n",
    "- __bpath__: el directorio donde almacenar la mejor de las redes de segmentación.\n",
    "- __num_classes__: el número de clases para la segmentación.\n",
    "- __num_epochs__: el número de *epochs* durante los que entrenar.\n",
    "\n",
    "__**IMPORTANTE:__ normalmente, cuando se realiza un procedimiento de _fine-tuning_ sobre una red ya pre-entrenada, la base de datos de que se dispone es pequeña y no se dispone de gran capacidad de computación (que permitiría usar *batches* más grandes). En estos casos es una buena práctica fijar los módulos de normalización de *batch* de la red (poniéndolos en modo `eval`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p6YuQoTWT1Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, jaccard_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "# We set the batchnorm modules to eval\n",
    "def set_bn_eval(mm):\n",
    "    if isinstance(mm, torch.nn.modules.batchnorm._BatchNorm):\n",
    "        mm.eval()\n",
    "        \n",
    "def train_model(model, criterion, dataloaders, device, optimizer, lr_scheduler, metrics, bpath, model_name, num_classes=2, num_epochs=3):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_jaccard = 0\n",
    "    # Initialize the log file for training and testing loss and metrics\n",
    "    fieldnames = ['epoch', 'Train_loss', 'Val_loss'] + \\\n",
    "        [f'Train_{m}' for m in metrics.keys()] + \\\n",
    "        [f'Val_{m}' for m in metrics.keys()]\n",
    "    with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if os.path.exists(os.path.join(bpath,model_name+'-epoch{}.pth'.format(epoch))):\n",
    "            print(\"=> loading checkpoint '{}'\".format(epoch))\n",
    "            checkpoint = torch.load(os.path.join(bpath,model_name+'-epoch{}.pth'.format(epoch)))\n",
    "            lr_scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            best_jaccard=checkpoint['best_jaccard']\n",
    "            print(\"=> loaded checkpoint '{}'\" .format(epoch))\n",
    "        else:\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "            print('-' * 10)\n",
    "            # Each epoch has a training and validation phase\n",
    "            # Initialize batch summary\n",
    "            batchsummary = {a: [0] for a in fieldnames}\n",
    "\n",
    "            for phase in ['Train', 'Val']:\n",
    "                if phase == 'Train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                model.apply(set_bn_eval)\n",
    "\n",
    "                # Iterate over data.\n",
    "                for sample in tqdm(iter(dataloaders[phase])):\n",
    "                    inputs = sample['image'].to(device)\n",
    "                    masks = sample['mask'].to(device)\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'Train'):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs['out'], masks)\n",
    "                        \n",
    "                        y_pred = outputs['out']#.data.cpu().numpy().ravel()\n",
    "                        y_prob = torch.nn.functional.softmax(y_pred, dim=1)\n",
    "                        _,y_pred = torch.max(y_prob,dim=1)\n",
    "                        y_pred = y_pred.data.cpu().numpy().ravel()\n",
    "                        y_prob = np.reshape(np.transpose(y_prob.data.cpu().numpy(),(1,0,2,3)),(num_classes,y_pred.shape[0]))\n",
    "                        y_true = masks.data.cpu().numpy()#.ravel()\n",
    "                        if (num_classes==2):\n",
    "                            y_true = y_true>0\n",
    "                        lb = LabelBinarizer()\n",
    "                        lb.fit([f for f in range(0,num_classes)])\n",
    "                        for name, metric in metrics.items():\n",
    "                            if name == 'jaccard_score':\n",
    "                                if (num_classes==2):\n",
    "                                    ji=metric(y_true[:,1,:,:].reshape(-1,1), y_pred, average=None)\n",
    "                                else:\n",
    "                                    ji=metric(y_true.reshape(-1,1), y_pred, labels=np.unique(y_true),average=None)\n",
    "                                batchsummary[f'{phase}_{name}'].append(\n",
    "                                    np.mean(ji[1:]))\n",
    "                            else:\n",
    "                                if (num_classes==2):\n",
    "                                    batchsummary[f'{phase}_{name}'].append(\n",
    "                                        metric(y_true[:,1,:,:].reshape(-1,1), y_prob[1:,:].T, average='micro',multi_class='ovr'))\n",
    "                                else:\n",
    "                                    batchsummary[f'{phase}_{name}'].append(\n",
    "                                        metric(lb.transform(y_true.reshape(-1,1)), y_prob.T, average='micro',multi_class='ovr'))\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'Train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                batchsummary['epoch'] = epoch\n",
    "                epoch_loss = loss\n",
    "                batchsummary[f'{phase}_loss'] = epoch_loss.item()\n",
    "                print('{} Loss: {:.4f}'.format(\n",
    "                    phase, loss))\n",
    "            for field in fieldnames[3:]:\n",
    "                batchsummary[field] = np.mean(batchsummary[field])\n",
    "            print(batchsummary)\n",
    "            with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow(batchsummary)\n",
    "                # deep copy the model if the jaccard is the best\n",
    "                if phase == 'Val' and batchsummary['Val_jaccard_score'] > best_jaccard:\n",
    "                    best_jaccard = batchsummary['Val_jaccard_score']\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save({'state_dict':best_model_wts}, os.path.join(bpath, model_name+'_best.pth.tar'))\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            # Save the state  \n",
    "            state = {'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict(),\n",
    "                      'scheduler':lr_scheduler.state_dict(), \n",
    "                      'best_jaccard': best_jaccard, }\n",
    "            torch.save(state, os.path.join(bpath, model_name+'-epoch{}.pth'.format(epoch)))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Highest Jaccard: {:4f}'.format(best_jaccard))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYYJVPrvWT1b"
   },
   "source": [
    "En el caso de la segmentación binaria, la salida de la red es una imagen con dos canales: uno para el *background* y otro para la clase a segmentar. El criterio para el entrenamiento de la red es la función `BCEWithLogitsLoss` para clasificación binaria.\n",
    "\n",
    "El entrenamiento de la red se realiza durante 12 epochs, reduciendo la tasa de entrenamiento a medida que se avanza en el entrenamiento. El código produce un archivo denominado _log.csv_ donde se puede analizar la variación de las funciones de pérdida en cada _epoch_ de entrenamiento, así como la correspondiente precisión y recall en el conjunto de test tanto para detección como para clasificación. Compruebe que las funciones de pérdida son algo ruidosas, y que las medidas de evaluación (área bajo la curva ROC e índice Jaccard) en validación van creciendo a medida que avanza el entrenamiento.\n",
    "\n",
    "__**IMPORTANTE:__ note cómo el área ROC no es descriptiva de cómo avanza el proceso de entrenamiento para nuestro caso (desde el principio es muy elevada, por encima del 95%) y no está correlada con el índice Jaccard. Esto ocurre por el __desbalanceo__ de la base de datos (la cúpula óptica representa una parte muy pequeña de las imágenes). En bases de datos muy desbalanceadas la curva ROC no es un buen indicativo de la eficiencia de la segmentación ya que la importancia que da a las clases depende de su probabilidad de aparición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17811,
     "status": "error",
     "timestamp": 1587802390366,
     "user": {
      "displayName": "MIGUEL MOLINA MORENO",
      "photoUrl": "",
      "userId": "15687095164361239457"
     },
     "user_tz": -120
    },
    "id": "1fnYCC-FWT1d",
    "outputId": "aa3b6508-a3a2-4aea-e288-f1855357385a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# custom weight initialization \n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight,1.0)\n",
    "        \n",
    "model_name='deeplabv3_binary'\n",
    "\n",
    "# Training stage\n",
    "model.train()\n",
    "\n",
    "# Create the experiment directory if not present\n",
    "if not os.path.isdir(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "# Specify the loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean') # 2 classes\n",
    "\n",
    "# Specify the optimizer with a lower learning rate for backbone\n",
    "params_classifier = [p for p in model.classifier.parameters() if p.requires_grad]\n",
    "\n",
    "params_backbone = [p for p in model.backbone.parameters() if p.requires_grad]\n",
    "\n",
    "# Initialize the classifier-conv_layer weights, to adapt to the new paradigm (retinography vs natural images)\n",
    "torch.manual_seed(manualSeed)\n",
    "model.classifier.apply(weights_init)\n",
    "\n",
    "# We apply a different lr to backbone and classifier parts\n",
    "optimizer = torch.optim.Adam([\n",
    "{'params': params_backbone},\n",
    "{'params': params_classifier, 'lr': 1e-3}\n",
    "], lr=1e-4)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=step_size,\n",
    "                                          gamma=0.1)\n",
    "\n",
    "\n",
    "# Specify the evaluation metrics\n",
    "metrics = {'jaccard_score': jaccard_score, 'auroc': roc_auc_score}\n",
    "\n",
    "trained_model = train_model(model, criterion, dataloaders, device,\n",
    "                        optimizer, lr_scheduler, bpath=result_dir, model_name=model_name,\n",
    "                        metrics=metrics, num_classes=num_classes_binary, num_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyzcghcRWT1h"
   },
   "source": [
    "### Evaluación\n",
    "\n",
    "Tras entrenar la red, se van a evaluar los resultados para el conjunto de test. En primer lugar, se cargan los pesos de la red entrenada en el modelo y se llama a la función de evaluación. Preste atención a los parámetros que recibe la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBZsqfy8WT1h"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "dataloaders = get_dataloader_riga(\n",
    "    data_dir, maskFolder='masks', binary=True, use_only_train=use_only_train, batch_size=batchsize_test, img_size=img_size, data_augm=False, image_mean=mean,image_std=std)\n",
    "weights=torch.load(os.path.join(result_dir,model_name+'_best.pth.tar'))['state_dict']\n",
    "model = get_deeplabv3(num_classes_binary)\n",
    "model.to(device)\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "test_segmentation_model_binary(model, dataloaders, class_names_binary, 0.5, result_dir, True, batchsize_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTcPO_PbWT1m"
   },
   "source": [
    "## Parte 3. Experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVlcbA25WT1n"
   },
   "source": [
    "### 1. Influencia del umbral en la inferencia.\n",
    "\n",
    "Analice la influencia del umbrales TH para los resultados en inferencia para el problema de segmentación binaria, tanto visualmente (para ello ponga el parámetro `SAVE_OPT` a `True`) como con las medidas de segmentación. A la vista de los resultados, ¿ha aprendido la red a segmentar de manera coherente las imágenes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8Ss8QWlWT1n"
   },
   "outputs": [],
   "source": [
    "test_segmentation_model_binary(model, dataloaders, class_names_binary, 0.2, result_dir, False, batchsize_test)\n",
    "test_segmentation_model_binary(model, dataloaders, class_names_binary, 0.8, result_dir, False, batchsize_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmfpipQ5WT1r"
   },
   "source": [
    "### 2. Utilidad de las capas *atrous* y el ASPP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geKhUnj4WT1r"
   },
   "source": [
    "En este experimento, para poder observar la capacidad de representación del ASPP con capas *atrous*, se construye una red similar a Deeplab-V2 sobre la arquitectura que tenemos. En Deeplab-V2 las capas *atrous* se encuentran justamente por delante de la función de *loss*, de manera que sus activaciones están __directamente relacionadas__ con la segmentación de salida de la red. En nuestra red sin embargo, las capas *atrous* están __más alejadas__ de la función de *loss* de manera que es __más difícil analizar sus activaciones__. Para convertir nuestra arquitectura en la de Deeplab-V2, se realiza lo siguiente:\n",
    "\n",
    "- Se elimina el bloque ASPP de Deeplab-V3, que contiene el *avg pooling*.\n",
    "- Se añade el bloque ASPP de Deeplab-V2, con 4 convoluciones *atrous* de tasa $r=6, 12, 18$ y $24$. A las salidas de cada rama del ASPP se coloca una capa de convolución que convierte cada mapa de características al tamaño del mapa de salida (según el número de clases, en nuestro caso 2, *background* + cúpula óptica). \n",
    "- Se suman las contribuciones de cada rama para generar la salida y se aplica la función de *loss*.\n",
    "\n",
    "La siguiente figura muestra los cambios que se realizan para visualizar las activaciones.\n",
    "\n",
    "<img src=\"https://tsc.uc3m.es/~mmolina/images_segmentation/deeplabv3v2.PNG\" width=\"600pix\">\n",
    "\n",
    "De este modo, se pueden visualizar las salidas marginales de cada rama y ver cómo funcionan las convoluciones *atrous* a partir de sus activaciones.\n",
    "\n",
    "- Analice las activaciones de cada rama para las imágenes de test. ¿Qué diferencias encuentra y a qué se deben? ¿Cómo contribuyen las convoluciones *atrous* y el ASPP a mejorar la segmentación? ¿En qué caso cree que serán más útiles, cuando la segmentación sea densa (existen muchos objetos en la imagen), o en un caso como este?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQ12_xMgWT1s",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Sum(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(Sum, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "    def forward(self, x):\n",
    "        x=x.unsqueeze(1)\n",
    "        x=x.view(x.size(0),4,self.num_classes,x.size(3),x.size(4))\n",
    "        x=torch.sum(x,dim=1)\n",
    "        return x\n",
    "\n",
    "def get_deeplabv2(num_classes=1):\n",
    "    model=get_deeplabv3(num_classes=2)\n",
    "    # Remove the last avg pooling\n",
    "    model.classifier[0].convs=model.classifier[0].convs[:-1]\n",
    "    # Change the convolutions from the remaining blocks r=6,12,28 and 24 and adding conv-layers to each branch\n",
    "    modules = []\n",
    "    modules.append(torch.nn.Conv2d(2048,256, kernel_size=3,stride=1,padding=6,dilation=6))\n",
    "    modules.append(model.classifier[0].convs[0][1])\n",
    "    modules.append(model.classifier[0].convs[0][2])\n",
    "    modules.append(torch.nn.Conv2d(256, num_classes, kernel_size=1))\n",
    "    model.classifier[0].convs[0]=torch.nn.Sequential(*modules)\n",
    "    modules = []\n",
    "    modules.append(torch.nn.Conv2d(2048,256, kernel_size=3,stride=1,padding=12,dilation=12))\n",
    "    modules.append(model.classifier[0].convs[1][1])\n",
    "    modules.append(model.classifier[0].convs[1][2])\n",
    "    modules.append(torch.nn.Conv2d(256, num_classes, kernel_size=1))\n",
    "    model.classifier[0].convs[1]=torch.nn.Sequential(*modules)\n",
    "    modules = []\n",
    "    modules.append(torch.nn.Conv2d(2048,256, kernel_size=3,stride=1,padding=18,dilation=18))\n",
    "    modules.append(model.classifier[0].convs[2][1])\n",
    "    modules.append(model.classifier[0].convs[2][2])\n",
    "    modules.append(torch.nn.Conv2d(256, num_classes, kernel_size=1))\n",
    "    model.classifier[0].convs[2]=torch.nn.Sequential(*modules)\n",
    "    modules = []\n",
    "    modules.append(torch.nn.Conv2d(2048,256, kernel_size=3,stride=1,padding=24,dilation=24))\n",
    "    modules.append(model.classifier[0].convs[3][1])\n",
    "    modules.append(model.classifier[0].convs[3][2])\n",
    "    modules.append(torch.nn.Conv2d(256, num_classes, kernel_size=1))\n",
    "    model.classifier[0].convs[3]=torch.nn.Sequential(*modules)\n",
    "    # Sum the marginal predictions for each branch\n",
    "    model.classifier[0].project=Sum(num_classes_binary)\n",
    "    model.classifier=model.classifier[0]\n",
    "    return model\n",
    "\n",
    "model_name='deeplabv2'\n",
    "\n",
    "dataloaders = get_dataloader_riga(\n",
    "    data_dir, maskFolder='masks', binary=True, use_only_train=use_only_train, batch_size=batchsize_train, img_size=img_size, data_augm=False, image_mean=mean,image_std=std)\n",
    "model = get_deeplabv2(num_classes_binary)\n",
    "# Use gpu if available\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "params_classifier = [p for p in model.classifier.parameters() if p.requires_grad]\n",
    "\n",
    "params_backbone = [p for p in model.backbone.parameters() if p.requires_grad]\n",
    "\n",
    "torch.manual_seed(manualSeed)\n",
    "model.classifier.apply(weights_init)\n",
    "\n",
    "\n",
    "# Specify the loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean') # 2 classes\n",
    "\n",
    "# Specify the optimizer with a lower learning rate\n",
    "optimizer = torch.optim.Adam([\n",
    "{'params': params_backbone},\n",
    "{'params': params_classifier, 'lr': 1e-3}\n",
    "], lr=1e-4)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=step_size,\n",
    "                                          gamma=0.1)\n",
    "\n",
    "# Specify the evaluation metrics\n",
    "metrics = {'jaccard_score': jaccard_score, 'auroc': roc_auc_score}\n",
    "\n",
    "trained_model = train_model(model, criterion, dataloaders, device,\n",
    "                        optimizer, lr_scheduler, bpath=result_dir, model_name=model_name,\n",
    "                        metrics=metrics, num_classes=num_classes_binary, num_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-dJQSciWT1w"
   },
   "source": [
    "__**IMPORTANTE:__ la función externa `get_activations` proporciona una representación visual de las salidas del ASPP, que almacena en la carpeta `activations` en el directorio de resultados. Sus entradas son:\n",
    "\n",
    "- __model__: la CNN que evaluar.\n",
    "- __dataloaders__: el cargador de los datos de test.\n",
    "- __device__: el dispositivo que utilizar para el entrenamiento (GPU o CPU).\n",
    "- __result_dir__: el directorio de resultados.\n",
    "- __batchsize_test__: el tamaño de batch para test, que debe ser 1.\n",
    "\n",
    "__**IMPORTANTE:__ la función `get_activations` hace uso de los *hook* de Pytorch. Los *hook* de Pytorch son una serie de funciones que permiten modificar los datos de entrada o salida de alguna capa de la red durante la ejecución de la red. Esto permite tener acceso a datos intermedios de la red en tiempo de ejecución, así como realizar un *debug* controlado de la red neuronal. En concreto se usa un *register_forward_hook()*, se ejecuta tras el método *forward* de cualquier capa de la red y tiene acceso a sus entradas y salidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2UlHEZEWT1w"
   },
   "outputs": [],
   "source": [
    "from external import get_activations\n",
    "# Code to obtain the activations\n",
    "dataloaders = get_dataloader_riga(\n",
    "data_dir, maskFolder='masks', binary=True, use_only_train=use_only_train, batch_size=batchsize_test, img_size=img_size, data_augm=False, image_mean=mean,image_std=std)\n",
    "weights=torch.load(os.path.join(result_dir,model_name+'_best.pth.tar'))['state_dict']\n",
    "model = get_deeplabv2(num_classes_binary)\n",
    "model.to(device)\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "get_activations(model, dataloaders, device, result_dir, batchsize_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h98EzCSeWT1z"
   },
   "source": [
    "### 3. Segmentación multi-clase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQrgDcqcWT10"
   },
   "source": [
    "En este apartado se va a abordar la segmentación multi-clase para la tarea que se propone, con las clases cúpula óptica y disco óptico. En primer lugar, se modifica la función de testeo de la segmentación, que ya no admite umbralizar los resultados (en el caso de la segmentación multi-clase es el valor máximo del `softmax` el que proporciona la clase predicha). En cualquier caso, se podrían aplicar pesos al entrenamiento para primar unas clases sobre otras. La función almacena los resultados de las predicciones en el directorio `predictions_multiclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKJVy_obWT11"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def test_segmentation_model_multiclass(model, dataloaders, num_classes, class_names, result_dir, SAVE_OPT, batch_size=1):\n",
    "    cm_total=np.zeros(len(class_names))\n",
    "    # Evaluation: Jaccard Index\n",
    "    jaccard=[]\n",
    "    # We create the results folder and CSV file for results if they do not exist\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    else:\n",
    "        if os.path.exists(os.path.join(result_dir,'results_multiclass.csv')):\n",
    "            os.remove(os.path.join(result_dir,'results_multiclass.csv'))\n",
    "    if (SAVE_OPT):\n",
    "        # We create the folder for predictions\n",
    "        if not os.path.exists(os.path.join(result_dir,'predictions_multiclass')):\n",
    "            os.mkdir(os.path.join(result_dir,'predictions_multiclass'))\n",
    "    csv_file=open(os.path.join(result_dir,'results_multiclass.csv'),'w')\n",
    "    coord_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for sample in dataloaders['Test']:\n",
    "        with torch.no_grad():\n",
    "            inputs = sample['image'].to(device)\n",
    "            masks = sample['mask'].to(device)\n",
    "            prediction = model(inputs)\n",
    "            y_pred = prediction['out']\n",
    "            y_pred = torch.nn.functional.softmax(y_pred, dim=1)\n",
    "            _,y_pred = torch.max(y_pred, dim=1)\n",
    "            y_pred = y_pred.data.cpu().numpy()\n",
    "            y_true = masks.data.cpu().numpy()\n",
    "            for j in range(y_pred.shape[0]):\n",
    "                img_name=sample['img_path'][j].split(os.path.sep)[-1]\n",
    "                dataset_name=sample['img_path'][j].split(os.path.sep)[-2]\n",
    "                ji=np.zeros((num_classes-1,),dtype='float')\n",
    "                for i in range(num_classes-1):\n",
    "                    # We measure with the Jaccard Index\n",
    "                    ji[i]=np.sum(np.logical_and(np.squeeze(y_pred[j,:,:]==(i+1)),np.squeeze(y_true==(i+1)))/(np.sum(y_pred[j,:,:]==(i+1))+np.sum(y_true[j,:,:]==(i+1))-np.sum(np.logical_and(np.squeeze(y_pred[j,:,:]==(i+1)),np.squeeze(y_true[j,:,:]==(i+1))))))\n",
    "                jaccard.append(ji)\n",
    "                cm_total=cm_total+confusion_matrix(y_true[j,:,:].reshape(-1,1), y_pred[j,:,:].reshape(-1,1), labels=None, sample_weight=None, normalize=None)\n",
    "                if (SAVE_OPT):\n",
    "                    img=Image.open(sample['img_path'][j]).resize((img_size,img_size))\n",
    "                    mask=(np.transpose(np.concatenate(((y_pred[j,:,:])[np.newaxis,:,:],y_true[j,:,:][np.newaxis,:,:],np.zeros_like(y_pred[j,:,:][np.newaxis,:,:])),axis=0),(1,2,0))*255.0/num_classes).astype(np.uint8)\n",
    "                    img = Image.blend(img.convert('RGBA'), Image.fromarray(mask).convert('RGBA'),0.5)\n",
    "                    img.save(os.path.join(result_dir,'predictions_multiclass',img_name[:-4]+'.png'))\n",
    "                coord_writer.writerow([img_name,str(ji)])\n",
    "    # Mean values\n",
    "    coord_writer.writerow(['MEAN',str(sum(jaccard)/len(jaccard))])\n",
    "    print('Jaccard index for segmentation: {}'.format(sum(jaccard)/len(jaccard)))\n",
    "    csv_file.close()\n",
    "    return cm_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlTremV3WT14"
   },
   "outputs": [],
   "source": [
    "model = get_deeplabv3(num_classes_multiclass)\n",
    "# Use gpu if available\n",
    "model.to(device)\n",
    "\n",
    "model_name='deeplabv3_multiclass'\n",
    "\n",
    "# Multi-class database\n",
    "dataloaders = get_dataloader_riga(\n",
    "    data_dir, maskFolder='masks_full', binary=False, use_only_train=use_only_train, batch_size=batchsize_train, img_size=img_size, data_augm=False, image_mean=mean,image_std=std)\n",
    "\n",
    "# Training stage\n",
    "model.train()\n",
    "\n",
    "# Create the experiment directory if not present\n",
    "if not os.path.isdir(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "# Specify the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')# 3 classes\n",
    "\n",
    "# Specify the optimizer with a lower learning rate for backbone\n",
    "params_classifier = [p for p in model.classifier.parameters() if p.requires_grad]\n",
    "\n",
    "params_backbone = [p for p in model.backbone.parameters() if p.requires_grad]\n",
    "\n",
    "# Initialize the classifier-conv_layer weights, to adapt to the new paradigm (retinography vs natural images)\n",
    "torch.manual_seed(manualSeed)\n",
    "model.classifier.apply(weights_init)\n",
    "\n",
    "# We apply a different lr to backbone and classifier parts\n",
    "optimizer = torch.optim.Adam([\n",
    "{'params': params_backbone},\n",
    "{'params': params_classifier, 'lr': 1e-3}\n",
    "], lr=1e-4)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=step_size,\n",
    "                                          gamma=0.1)\n",
    "\n",
    "\n",
    "# Specify the evaluation metrics\n",
    "metrics = {'jaccard_score': jaccard_score, 'auroc': roc_auc_score}\n",
    "\n",
    "trained_model = train_model(model, criterion, dataloaders, device,\n",
    "                        optimizer, lr_scheduler, bpath=result_dir, model_name=model_name,\n",
    "                        metrics=metrics, num_classes=num_classes_multiclass, num_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9C-FF8tWT17"
   },
   "source": [
    "Efectúe la inferencia para el modelo y analice los resultados. \n",
    "\n",
    "- ¿Qué ocurre al introducir la nueva clase sobre el índice Jaccard de la cúpula óptica?\n",
    "- Analice la matriz de confusión. ¿Qué clases es más sencillo confundir entre sí? Justifíquelo debido a la topología de las clases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGIqH5bZWT18"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "dataloaders = get_dataloader_riga(\n",
    "    data_dir, maskFolder='masks_full', binary=False, use_only_train=use_only_train, batch_size=batchsize_test, img_size=img_size, data_augm=False, image_mean=mean,image_std=std)\n",
    "weights=torch.load(os.path.join(result_dir,model_name+'_best.pth.tar'))['state_dict']\n",
    "model = get_deeplabv3(num_classes_multiclass)\n",
    "model.load_state_dict(weights)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "cm=test_segmentation_model_multiclass(model, dataloaders, num_classes_multiclass, class_names_multiclass, result_dir, True, batchsize_test)\n",
    "dconf=ConfusionMatrixDisplay(cm,display_labels=class_names_multiclass)\n",
    "dconf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGzTd88PWT1-"
   },
   "source": [
    "### 4. Funciones de pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUnNSF8iWT1_"
   },
   "source": [
    "En este apartado se van a describir situaciones  con la función de pérdida de la red para posteriores experimentos. A la hora de entrenar una red de segmentación, hay que tener en cuenta dos cosas principalmente: el desbalanceo de las clases; si existe, y el uso de estrategias de regularización de la función de pérdida pixelar. Ambas se definen a continuación.\n",
    "\n",
    "#### Desbalanceo de las clases\n",
    "\n",
    "El desbalanceo de las clases ocurre cuando algunas de las clases en la salida dominan sobre el resto (es decir, la proporción de píxeles pertenecientes a las distintas clases es muy desigual). Esto puede provocar que la red tienda a dar demasiada importancia a las clases más representadas a costa de reducir la importancia (o incluso hacer desaparecer) clases poco representadas. Esto se puede comprender mejor si se analiza a nivel de píxel: __la función de pérdida estándar aplica la misma importancia a todos los píxeles de la salida, independientemente de su clase, es decir, la red se va a centrar en clasificar cada uno de ellos correctamente, sea cual sea su clase. Sin embargo, si existe desbalanceo para las clases, un error en una clase poco representada (1 error sobre 10 píxeles, por ejemplo) será más perjudicial para el rendimiento del sistema que un error en una clase muy representada (1/1000)__. Si se une esto a que la red puede aprender mejor la clase muy representada porque tiene un mayor número de ejemplos de la misma, el resultado de la segmentación puede ser poco preciso\n",
    "\n",
    "Como estrategia para mitigar esto se propone una muy sencilla: se basa en aplicar una serie de pesos a las clases en la función de pérdida según su probabilidad de aparición en el conjunto de entrenamiento. Esto provoca que la red no ponga el mismo énfasis en clasificar cada píxel, sino que un error en un píxel de una clase poco representada dará un valor de la función de pérdida mayor que en una clase muy representada, de manera que la red pone más énfasis en resolver los errores sobre la clase más \"difícil\". Sin embargo, esta estrategia es sensible a los pesos que se apliquen a las clases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIWY0eyQWT1_"
   },
   "source": [
    "#### Estrategias de regularización\n",
    "\n",
    "Por otra parte, la función de pérdida pixelar no tiene en cuenta ninguna dependencia entre los píxeles de la imagen (es decir, a la función contribuyen de igual manera todos los píxeles de la imagen; y entre ellos no se impone ninguna relación). Sin embargo, en el caso de la segmentación de imágenes existe una fuerte dependencia entre los píxeles:\n",
    "\n",
    "- A nivel local, píxeles adyacentes a uno clasificado como perteneciente a la clase 'X' por ejemplo, tienen más probabilidad de ser de la clase 'X' que de la clase 'Y' en imágenes naturales (los objetos son continuos hasta que se llega a sus bordes). Existen estrategias sobre la función de pérdida que pueden reforzar la coherencia local en las segmentaciones (véase *'pairwise penalties'* en [6]).\n",
    "- A nivel de imagen, existirán imágenes que la red segmente correctamente y otras en las que el desempeño sea menor. Existen funciones de pérdida que modelan esto (hacen que la red se centre en las imágenes más complejas) y combinadas con la función pixelar pueden resultar de ayuda, como la Dice Loss, en [8].\n",
    "- En objetos no uniformes (con regiones con aspectos muy diferentes), pueden existir ciertos tipos de regiones que la red segmente muy bien y otras que sean complejas. Una función de pérdida que dé mayor importancia a los píxeles mal clasificados que a los correctamente clasificados puede ser útil. De este modo la red se centrará en clasificar correctamente dichos píxeles (lo que frecuentemente no afecta a los píxeles ya correctamente clasificados, que son más sencillos) y los resultados mejorarán. Esto equivale a hacer un procedimiento de *Hard Negative Mining* sobre los píxeles (es decir, centrarse en aquellos que resultan más difíciles para la red). La función Focal Loss [7] puede ser un buen ejemplo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6ITFzeBWT2B"
   },
   "source": [
    "### 4. Evaluación del trabajo autónomo del alumno.\n",
    "\n",
    "#### Criterios de evaluación\n",
    "\n",
    "De esta práctica (si elegida) surge la segunda evaluación para la asignatura. Una vez comprendidos los fundamentos de la red de segmentación, puede realizar los experimentos que considere oportunos. Estos experimentos pueden ir dirigidos a:\n",
    "\n",
    "- Profundizar en la arquitectura (observar la dependencia de los resultados con las modificaciones de la misma, especialmente en la parte del clasificador: convoluciones *atrous*).\n",
    "- Analizar los resultados y proporcionar estrategias de mejora.\n",
    "- Modificar el proceso de entrenamiento de la red a través de la función de pérdida, con las estrategias descritas en el apartado anterior u otras.\n",
    "\n",
    "\n",
    "#### Entregables\n",
    "\n",
    "- Presentación (Fecha indicada en la entrega del proyecto en Aula Global). Este día cada grupo de alumnos tendrá un turno de 10 minutos de preguntas (máximo 5 minutos de presentación) sobre el apartado de trabajo autónomo con ayuda de un máximo de 3 transparencias.\n",
    "- Informe + Código. Los alumnos entregarán un breve informe (2 caras para la descripción, 1 cara de referencias y figuras si fuese necesaria) donde describirán los aspectos más importantes de la solución propuesta. El objetivo es que el alumno describa los análisis y extensiones que ha planteado al modelo y justifique su objetivo y utilidad de manera breve. Asimismo, se proporcionará el código utilizado para los experimentos (bien sobre este mismo Notebook, en formato `.ipynb` o bien en código Python, en formato `.py`). \n",
    "\n",
    "La fecha límite de entrega del fichero de código y el informe es la fecha indicada en la entrega del proyecto en Aula Global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNkW5LcEWT2B"
   },
   "source": [
    "#### Sugerencias\n",
    "\n",
    "A continuación se proporcionan algunas sugerencias para que el alumno trabaje de manera autónoma, a título informativo. Si lo desea, puede centrarse en implementar una o varias de ellas.\n",
    "\n",
    "- Se puede trabajar desde el punto de vista del *data augmentation*, efectuando un procedimiento __adecuado a alguna de las tareas de segmentación__ para ampliar la variabilidad de la base de datos.\n",
    "\n",
    "- Se puede modificar la estructura del clasificador de la red para adecuarlo a la tarea de segmentación con la que se trabaja (muestre especial atención al *receptive field* de la red). Además, se puede utilizar el bloque de clasificación auxiliar de la red (`aux_classifier`), modificando su estructura y añadiendo una función de pérdida para el caso de segmentación multi-clase. \n",
    "\n",
    "- A partir de las ideas proporcionadas en el apartado de funciones de pérdida, puede implementar alguna de las estrategias de regularización que se han descrito u otras que considere oportunas. Justifique su utilidad para la tarea que se propone.\n",
    "\n",
    "- Si lo desea, utilice otras bases de datos para segmentación binaria o multi-clase e implemente estrategias para mejorar los resultados en la tarea de segmentación de objetos en las mismas.\n",
    "\n",
    "__**IMPORTANTE:__ si encuentra problemas para entrenar la red Deeplab-V3 con backbone ResNet-101 o el entrenamiento es demasiado lento y desea agilizar los experimentos, el siguiente fragmento de código implementa Deeplab-V3 con un backbone mucho más ligero, de ResNet-18. Puede utilizarla como red *baseline* para el apartado autónomo. Tenga en cuenta que al no ser una red pre-entrenada en una base de datos para segmentación los resultados serán algo peores, especialmente en las activaciones *atrous* de la red, que visualmente no serán tan claras como en la red tratada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0V9dY_5ZWT2B"
   },
   "outputs": [],
   "source": [
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "def get_deeplabv3(num_classes=1):\n",
    "    model = deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "    backbone = resnet18(pretrained=True)\n",
    "    return_layers = {'layer4': 'out'}\n",
    "    return_layers['layer3'] = 'aux'\n",
    "    model.backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "    # Replace stride with dilation in blocks 3 and 4\n",
    "    model.backbone.layer3[0].conv1.stride=(1,1)\n",
    "    model.backbone.layer4[0].conv1.stride=(1,1)\n",
    "    model.backbone.layer3[0].conv1.dilation=(2,2)\n",
    "    model.backbone.layer3[0].conv1.padding=(2,2)\n",
    "    model.backbone.layer4[0].conv1.dilation=(4,4)\n",
    "    model.backbone.layer4[0].conv1.padding=(4,4)\n",
    "    model.backbone.layer3[0].downsample[0].stride=(1,1)\n",
    "    model.backbone.layer4[0].downsample[0].stride=(1,1)\n",
    "    \n",
    "    model.classifier = DeepLabHead(512, num_classes)\n",
    "    model.aux_classifier=FCNHead(256,num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**IMPORTANTE:__ es posible que la base de datos que se proporciona no constituya un buen *baseline* para los experimentos del apartado autónomo (la tarea de segmentación es bastante sencilla). Adicionalmente, se proporciona una base de datos de microscopía para una tarea más compleja, segmentación binaria de neutrófilos en vasos sanguíneos (en la carpeta __neutrophils__) o puede utilizar otras bases de datos disponibles."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
